{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textrank.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV_GUpXPEVbz",
        "colab_type": "text"
      },
      "source": [
        "#Text rank#\n",
        "To remember\n",
        "* cite resources appropriately later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoakRPvREh20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F0C9Qj9FJS-",
        "colab_type": "code",
        "outputId": "e39dfca8-ea73-4d17-b51a-0e8245b813ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# download and required modules of nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# setting up gdrive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDDkXMRyFW29",
        "colab_type": "text"
      },
      "source": [
        "#Dataset : tmp#\n",
        "Description,size\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cULA_qDF4Ks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing phase \n",
        "# will change later to more complex \n",
        "# this cell must be well abstracted\n",
        "\n",
        "\n",
        "# read the data as csv\n",
        "# data = pd.read_csv(\"content/gdrive/My Drive/submit\")\n",
        "data=pd.read_csv(\"/content/gdrive/My Drive/tennis_articles_v4.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpodsDr1Gh47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splitting into sentences\n",
        "sentences = []\n",
        "for s in data[\"article_text\"]:\n",
        "  sentences.extend(sent_tokenize(s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBqdS5ptMpeI",
        "colab_type": "code",
        "outputId": "7a801271-6944-4530-daf6-ef688a4f0ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "#print sample sentences\n",
        "_=[print(s) for s in sentences[:5]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maria Sharapova has basically no friends as tennis players on the WTA Tour.\n",
            "The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much.\n",
            "I think everyone knows this is my job here.\n",
            "When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\n",
            "I'm a pretty competitive girl.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_J6LbeXNIVz",
        "colab_type": "text"
      },
      "source": [
        "#Initialising Word Embeddings(Glove)#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_MVklIGNHhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment below commands to download zip\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove.6B.zip.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrk2OG4vN8bV",
        "colab_type": "code",
        "outputId": "00bd191c-54fd-46b7-ed4b-7965679890a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "word_emb = dict()\n",
        "with open('glove.6B.100d.txt',encoding='utf-8') as f:\n",
        "  for line in f :\n",
        "    tokens = line.strip().split()\n",
        "    word_emb[tokens[0]] = np.asarray(tokens[1:],dtype='float32')\n",
        "print(\"Initialised \"+str(len(word_emb))+\" embeddings\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialised 400000 embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kgvv8I7Q9ZQ",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing Stage#\n",
        "* Implement alternate methods for calculating sentence vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMRRH5ViRCYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text cleaning \n",
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\",\" \")\n",
        "clean_sentences = [s.lower() for s in clean_sentences]\n",
        "\n",
        "# remove stop_words\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n",
        "stop_words = stopwords.words('english')\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCuFQBdBU8BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vectorizing sentences\n",
        "# we take the average of word vectors as sentence vector\n",
        "# note that word vectors are of length 100\n",
        "zero_vec = np.zeros((100,))\n",
        "sentence_vectors = []\n",
        "for s in clean_sentences:\n",
        "  if len(s) >0 :\n",
        "    words = s.split()\n",
        "    avg_vec = np.mean([word_emb.get(w,zero_vec) for w in words],0)\n",
        "    sentence_vectors.append(avg_vec)\n",
        "  else:\n",
        "    sentence_vectors.append(zero_vec)\n",
        "\n",
        "# convert to np array\n",
        "sentence_vectors = np.array(sentence_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26hA6NJYZGrp",
        "colab_type": "text"
      },
      "source": [
        "#Similarity Matrix#\n",
        "* Implement cosine,unit overlap, and others"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkmyiZ7AZUCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_sentences = sentence_vectors.shape[0]\n",
        "similarity_matrix = np.zeros((num_sentences,)*2)\n",
        "for i in range(num_sentences):\n",
        "  for j in range(num_sentences):\n",
        "    if i==j:\n",
        "      # ignore self similarity\n",
        "      continue;\n",
        "    similarity_matrix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
        "                                                sentence_vectors[j].reshape(1,100))[0][0]\n",
        "\n",
        "                                                \n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfravKYVcdPy",
        "colab_type": "text"
      },
      "source": [
        "#Applying Ranking Algorithm#\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQhawpU6cosx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "scores = nx.pagerank(nx_graph)\n",
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUzI2YylfQ8c",
        "colab_type": "text"
      },
      "source": [
        "#Results#\n",
        "* print \n",
        "* rouge and other measures to evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k82_oB9afbzA",
        "colab_type": "code",
        "outputId": "8466547e-aa77-495d-d6bc-102f8c261b20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(ranked_sentences[i][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\n",
            "Major players feel that a big event in late November combined with one in January before the Australian Open will mean too much tennis and too little rest.\n",
            "Speaking at the Swiss Indoors tournament where he will play in Sundays final against Romanian qualifier Marius Copil, the world number three said that given the impossibly short time frame to make a decision, he opted out of any commitment.\n",
            "\"I felt like the best weeks that I had to get to know players when I was playing were the Fed Cup weeks or the Olympic weeks, not necessarily during the tournaments.\n",
            "Currently in ninth place, Nishikori with a win could move to within 125 points of the cut for the eight-man event in London next month.\n",
            "He used his first break point to close out the first set before going up 3-0 in the second and wrapping up the win on his first match point.\n",
            "The Spaniard broke Anderson twice in the second but didn't get another chance on the South African's serve in the final set.\n",
            "\"We also had the impression that at this stage it might be better to play matches than to train.\n",
            "The competition is set to feature 18 countries in the November 18-24 finals in Madrid next year, and will replace the classic home-and-away ties played four times per year for decades.\n",
            "Federer said earlier this month in Shanghai in that his chances of playing the Davis Cup were all but non-existent.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}